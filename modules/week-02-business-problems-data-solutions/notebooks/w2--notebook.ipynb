{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Thoughts: From Data to Business Action\n",
    "\n",
    "The journey from raw data to actionable business insights involves several key steps we've practiced today:\n",
    "\n",
    "1. **Data Understanding** ‚Üí Know your data quality, structure, and business context\n",
    "2. **Exploratory Analysis** ‚Üí Discover patterns and relationships in your data  \n",
    "3. **Model Selection** ‚Üí Choose appropriate techniques based on your business question\n",
    "4. **Interpretation** ‚Üí Translate technical results into business language\n",
    "5. **Action Planning** ‚Üí Convert insights into concrete business strategies\n",
    "\n",
    "### Practice Exercises (Try These!)\n",
    "\n",
    "1. **Classification Challenge**: Try predicting a different outcome using the same dataset\n",
    "2. **Clustering Experiment**: Use different numbers of clusters (k=3, k=5) and compare results  \n",
    "3. **Feature Engineering**: Create new features combining existing ones (e.g., ratios, categories)\n",
    "4. **Business Scenarios**: Apply these techniques to your own industry or organization\n",
    "\n",
    "### Resources for Continued Learning\n",
    "\n",
    "- **Python Practice**: Continue with basic Python tutorials and pandas documentation\n",
    "- **Business Analytics**: Focus on translating business problems to data questions\n",
    "- **Domain Knowledge**: The more you understand your business context, the better your analysis will be\n",
    "- **Experimentation**: Try different approaches and compare results\n",
    "\n",
    "Remember: The goal isn't just to run algorithms, but to generate insights that drive better business decisions!\n",
    "\n",
    "---\n",
    "\n",
    "**End of Notebook**\n",
    "\n",
    "*Happy analyzing! üöÄüìä*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anomaly Detection using Isolation Forest\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "print(\"Anomaly Detection for Business Intelligence\")\n",
    "print(\"=\"*45)\n",
    "\n",
    "# Use the same scaled data from clustering\n",
    "iso_forest = IsolationForest(contamination=0.1, random_state=42)  # Expect 10% anomalies\n",
    "anomaly_labels = iso_forest.fit_predict(scaled_data)\n",
    "\n",
    "# Convert to binary labels (1 = normal, 0 = anomaly)\n",
    "anomaly_binary = (anomaly_labels == 1).astype(int)\n",
    "n_anomalies = sum(anomaly_labels == -1)\n",
    "\n",
    "print(f\"Detected {n_anomalies} anomalies out of {len(anomaly_labels)} samples\")\n",
    "print(f\"Anomaly rate: {n_anomalies/len(anomaly_labels)*100:.1f}%\")\n",
    "\n",
    "# Add anomaly information to our dataframe\n",
    "df_with_anomalies = df_clustered.copy()\n",
    "df_with_anomalies['Is_Anomaly'] = (anomaly_labels == -1)\n",
    "\n",
    "# Analyze anomalies\n",
    "print(f\"\\nAnomalous Data Points:\")\n",
    "print(\"-\" * 25)\n",
    "\n",
    "anomalous_data = df_with_anomalies[df_with_anomalies['Is_Anomaly'] == True]\n",
    "\n",
    "if len(anomalous_data) > 0:\n",
    "    print(\"Characteristics of anomalous data points:\")\n",
    "    for feature in clustering_features[:3]:  # Show top 3 features\n",
    "        normal_mean = df_with_anomalies[df_with_anomalies['Is_Anomaly'] == False][feature].mean()\n",
    "        anomaly_mean = anomalous_data[feature].mean()\n",
    "        print(f\"  {feature}:\")\n",
    "        print(f\"    Normal: {normal_mean:.2f}\")\n",
    "        print(f\"    Anomalies: {anomaly_mean:.2f}\")\n",
    "        print(f\"    Difference: {anomaly_mean - normal_mean:.2f}\")\n",
    "\n",
    "    # Visualize anomalies\n",
    "    if len(clustering_features) >= 2:\n",
    "        plt.figure(figsize=(12, 5))\n",
    "        \n",
    "        # Plot 1: Anomalies in feature space\n",
    "        plt.subplot(1, 2, 1)\n",
    "        \n",
    "        if len(clustering_features) > 2:\n",
    "            # Use PCA data if available\n",
    "            normal_points = pca_data[anomaly_labels == 1]\n",
    "            anomaly_points = pca_data[anomaly_labels == -1]\n",
    "            \n",
    "            plt.scatter(normal_points[:, 0], normal_points[:, 1], \n",
    "                       c='blue', alpha=0.6, label='Normal', s=20)\n",
    "            plt.scatter(anomaly_points[:, 0], anomaly_points[:, 1], \n",
    "                       c='red', alpha=0.8, label='Anomaly', s=60, marker='x')\n",
    "            plt.xlabel('PC1')\n",
    "            plt.ylabel('PC2')\n",
    "        else:\n",
    "            feature1, feature2 = clustering_features[0], clustering_features[1]\n",
    "            normal_data = df_with_anomalies[df_with_anomalies['Is_Anomaly'] == False]\n",
    "            anomalous_data = df_with_anomalies[df_with_anomalies['Is_Anomaly'] == True]\n",
    "            \n",
    "            plt.scatter(normal_data[feature1], normal_data[feature2], \n",
    "                       c='blue', alpha=0.6, label='Normal', s=20)\n",
    "            plt.scatter(anomalous_data[feature1], anomalous_data[feature2], \n",
    "                       c='red', alpha=0.8, label='Anomaly', s=60, marker='x')\n",
    "            plt.xlabel(feature1)\n",
    "            plt.ylabel(feature2)\n",
    "        \n",
    "        plt.title('Anomaly Detection Results')\n",
    "        plt.legend()\n",
    "        \n",
    "        # Plot 2: Anomaly distribution by cluster\n",
    "        plt.subplot(1, 2, 2)\n",
    "        \n",
    "        anomaly_by_cluster = df_with_anomalies.groupby('Cluster')['Is_Anomaly'].sum()\n",
    "        total_by_cluster = df_with_anomalies.groupby('Cluster').size()\n",
    "        anomaly_rate_by_cluster = (anomaly_by_cluster / total_by_cluster) * 100\n",
    "        \n",
    "        bars = anomaly_rate_by_cluster.plot(kind='bar', color='orange', alpha=0.7)\n",
    "        plt.title('Anomaly Rate by Customer Segment')\n",
    "        plt.xlabel('Cluster')\n",
    "        plt.ylabel('Anomaly Rate (%)')\n",
    "        plt.xticks(rotation=0)\n",
    "        \n",
    "        # Add value labels\n",
    "        for i, v in enumerate(anomaly_rate_by_cluster):\n",
    "            plt.text(i, v + max(anomaly_rate_by_cluster)*0.02, f'{v:.1f}%', \n",
    "                     ha='center', va='bottom')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(f\"\\nBusiness Applications of Anomaly Detection:\")\n",
    "        print(\"=\" * 45)\n",
    "        print(\"üîç Fraud Detection: Identify unusual transaction patterns\")\n",
    "        print(\"‚ö†Ô∏è  Quality Control: Detect products outside normal specifications\")\n",
    "        print(\"üìä Performance Monitoring: Spot unusual business metrics\")\n",
    "        print(\"üõ°Ô∏è  Risk Management: Identify high-risk customers or accounts\")\n",
    "        print(\"\\nüí° Next Step: Investigate these anomalies to understand if they represent:\")\n",
    "        print(\"   ‚Ä¢ Data entry errors that need correction\")\n",
    "        print(\"   ‚Ä¢ Genuinely unusual but legitimate cases\")\n",
    "        print(\"   ‚Ä¢ Potential fraud or problems requiring immediate attention\")\n",
    "\n",
    "else:\n",
    "    print(\"No anomalies detected in the current dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional Unsupervised Learning Example: Anomaly Detection\n",
    "\n",
    "Anomaly detection is another powerful unsupervised learning technique that can help businesses identify:\n",
    "- Fraudulent transactions\n",
    "- Unusual spending patterns  \n",
    "- Equipment failures\n",
    "- Quality control issues\n",
    "\n",
    "Let's implement a simple anomaly detection example to identify unusual patterns in our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Summary and Next Steps\n",
    "\n",
    "### What We've Learned Today\n",
    "\n",
    "In this notebook, we've covered the fundamentals of translating business problems into data science solutions:\n",
    "\n",
    "#### üêç Python Basics\n",
    "- **Variables & Calculations**: Storing and manipulating business data\n",
    "- **Lists & Dictionaries**: Organizing multiple data points\n",
    "- **Conditional Logic**: Making data-driven decisions  \n",
    "- **Loops**: Processing multiple records efficiently\n",
    "- **Functions**: Creating reusable business calculations\n",
    "\n",
    "#### üìä Data Analysis with Pandas\n",
    "- Loading and exploring business datasets\n",
    "- Understanding data structure and quality\n",
    "- Calculating summary statistics\n",
    "- Creating meaningful visualizations\n",
    "\n",
    "#### üéØ Supervised Learning (Classification)\n",
    "- **Business Application**: Predicting business success\n",
    "- **Models Used**: Logistic Regression and Random Forest\n",
    "- **Key Insight**: Understanding which factors most influence outcomes\n",
    "- **Business Value**: Make informed decisions about resource allocation\n",
    "\n",
    "#### üë• Unsupervised Learning (Clustering)\n",
    "- **Business Application**: Customer segmentation\n",
    "- **Method Used**: K-means clustering  \n",
    "- **Key Insight**: Identifying distinct customer groups\n",
    "- **Business Value**: Targeted marketing strategies and personalized services\n",
    "\n",
    "### Key Business Insights\n",
    "\n",
    "1. **Data-Driven Decisions**: Instead of relying on intuition alone, we can use historical data to predict future outcomes\n",
    "2. **Pattern Recognition**: Machine learning helps identify hidden patterns that might not be obvious to human analysis\n",
    "3. **Customer Understanding**: Segmentation reveals different customer types, enabling more effective business strategies\n",
    "4. **Predictive Power**: Models can help anticipate business outcomes and inform strategic planning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze cluster characteristics\n",
    "print(\"Customer Segment Analysis\")\n",
    "print(\"=\"*30)\n",
    "\n",
    "# Calculate cluster statistics\n",
    "cluster_stats = df_clustered.groupby('Cluster')[clustering_features].agg(['mean', 'std', 'count'])\n",
    "cluster_stats = cluster_stats.round(2)\n",
    "\n",
    "# Display cluster characteristics\n",
    "for cluster in sorted(df_clustered['Cluster'].unique()):\n",
    "    print(f\"\\nCluster {cluster} Profile:\")\n",
    "    print(\"-\" * 20)\n",
    "    cluster_data = df_clustered[df_clustered['Cluster'] == cluster]\n",
    "    print(f\"Size: {len(cluster_data)} customers ({len(cluster_data)/len(df_clustered)*100:.1f}%)\")\n",
    "    \n",
    "    # Show top characteristics for this cluster\n",
    "    if len(clustering_features) > 0:\n",
    "        print(\"Key characteristics:\")\n",
    "        for feature in clustering_features[:5]:  # Show top 5 features\n",
    "            mean_val = cluster_data[feature].mean()\n",
    "            overall_mean = df_clustered[feature].mean()\n",
    "            if abs(mean_val) > 0.01:  # Only show if meaningful value\n",
    "                if mean_val > overall_mean * 1.2:\n",
    "                    print(f\"  ‚ñ≤ High {feature}: {mean_val:.2f} (avg: {overall_mean:.2f})\")\n",
    "                elif mean_val < overall_mean * 0.8:\n",
    "                    print(f\"  ‚ñº Low {feature}: {mean_val:.2f} (avg: {overall_mean:.2f})\")\n",
    "                else:\n",
    "                    print(f\"  = Average {feature}: {mean_val:.2f}\")\n",
    "\n",
    "# Visualize cluster characteristics\n",
    "if len(clustering_features) >= 2:\n",
    "    # Use PCA for visualization if we have many features\n",
    "    if len(clustering_features) > 2:\n",
    "        pca = PCA(n_components=2)\n",
    "        pca_data = pca.fit_transform(scaled_data)\n",
    "        feature1, feature2 = 'PC1', 'PC2'\n",
    "        x_data, y_data = pca_data[:, 0], pca_data[:, 1]\n",
    "        \n",
    "        print(f\"\\nPCA Analysis:\")\n",
    "        print(f\"PC1 explains {pca.explained_variance_ratio_[0]:.1%} of variance\")\n",
    "        print(f\"PC2 explains {pca.explained_variance_ratio_[1]:.1%} of variance\")\n",
    "        print(f\"Total explained: {sum(pca.explained_variance_ratio_):.1%}\")\n",
    "        \n",
    "    else:\n",
    "        # Use first two features directly\n",
    "        feature1, feature2 = clustering_features[0], clustering_features[1]\n",
    "        x_data = df_clustered[feature1]\n",
    "        y_data = df_clustered[feature2]\n",
    "    \n",
    "    # Create cluster visualization\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    # Plot 1: Scatter plot of clusters\n",
    "    plt.subplot(1, 3, 1)\n",
    "    scatter = plt.scatter(x_data, y_data, c=cluster_labels, cmap='viridis', alpha=0.6)\n",
    "    plt.colorbar(scatter)\n",
    "    plt.title('Customer Segments Visualization')\n",
    "    plt.xlabel(feature1)\n",
    "    plt.ylabel(feature2)\n",
    "    \n",
    "    # Add cluster centers if using original features\n",
    "    if len(clustering_features) <= 2:\n",
    "        centers = kmeans.cluster_centers_\n",
    "        if len(clustering_features) == 2:\n",
    "            # Transform centers back to original scale\n",
    "            centers_original = scaler.inverse_transform(centers)\n",
    "            plt.scatter(centers_original[:, 0], centers_original[:, 1], \n",
    "                       c='red', marker='x', s=200, linewidth=3, label='Centers')\n",
    "            plt.legend()\n",
    "    \n",
    "    # Plot 2: Cluster size comparison\n",
    "    plt.subplot(1, 3, 2)\n",
    "    cluster_counts.plot(kind='pie', autopct='%1.1f%%', colors=plt.cm.viridis(np.linspace(0, 1, len(cluster_counts))))\n",
    "    plt.title('Cluster Size Distribution')\n",
    "    plt.ylabel('')\n",
    "    \n",
    "    # Plot 3: Business success by cluster (if isOpen exists)\n",
    "    plt.subplot(1, 3, 3)\n",
    "    if 'isOpen' in df_clustered.columns:\n",
    "        success_by_cluster = df_clustered.groupby('Cluster')['isOpen'].mean()\n",
    "        bars = success_by_cluster.plot(kind='bar', color='lightcoral')\n",
    "        plt.title('Business Success Rate by Cluster')\n",
    "        plt.xlabel('Cluster')\n",
    "        plt.ylabel('Success Rate')\n",
    "        plt.xticks(rotation=0)\n",
    "        plt.ylim(0, 1)\n",
    "        \n",
    "        # Add percentage labels\n",
    "        for i, v in enumerate(success_by_cluster):\n",
    "            plt.text(i, v + 0.02, f'{v:.1%}', ha='center', va='bottom')\n",
    "    else:\n",
    "        plt.text(0.5, 0.5, 'isOpen column\\nnot available', ha='center', va='center')\n",
    "        plt.title('Business Success by Cluster')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Create business personas for each cluster\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"BUSINESS PERSONAS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "personas = {\n",
    "    0: \"Conservative Customers\",\n",
    "    1: \"High-Value Customers\", \n",
    "    2: \"Growing Customers\",\n",
    "    3: \"Budget-Conscious Customers\"\n",
    "}\n",
    "\n",
    "for cluster in sorted(df_clustered['Cluster'].unique()):\n",
    "    cluster_name = personas.get(cluster, f\"Cluster {cluster}\")\n",
    "    cluster_size = len(df_clustered[df_clustered['Cluster'] == cluster])\n",
    "    \n",
    "    print(f\"\\nüéØ {cluster_name}\")\n",
    "    print(f\"   Size: {cluster_size} customers ({cluster_size/len(df_clustered)*100:.1f}% of total)\")\n",
    "    \n",
    "    if 'isOpen' in df_clustered.columns:\n",
    "        success_rate = df_clustered[df_clustered['Cluster'] == cluster]['isOpen'].mean()\n",
    "        print(f\"   Business Success Rate: {success_rate:.1%}\")\n",
    "    \n",
    "    print(\"   Marketing Strategy:\")\n",
    "    if cluster == 0:\n",
    "        print(\"   ‚Üí Focus on trust-building and reliability\")\n",
    "        print(\"   ‚Üí Offer stable, proven products\")\n",
    "    elif cluster == 1:\n",
    "        print(\"   ‚Üí Provide premium services and exclusive offers\")\n",
    "        print(\"   ‚Üí Focus on personalized experiences\")\n",
    "    elif cluster == 2:\n",
    "        print(\"   ‚Üí Nurture growth with educational content\")\n",
    "        print(\"   ‚Üí Offer scalable solutions\")\n",
    "    else:\n",
    "        print(\"   ‚Üí Emphasize value and cost-effectiveness\")\n",
    "        print(\"   ‚Üí Provide budget-friendly options\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Analyzing Customer Segments\n",
    "Let's examine the characteristics of each cluster to understand different customer types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import clustering libraries\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "print(\"Preparing Data for Customer Segmentation\")\n",
    "print(\"=\"*45)\n",
    "\n",
    "# Select features for clustering (use numeric columns)\n",
    "clustering_features = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "if 'isOpen' in clustering_features:\n",
    "    clustering_features.remove('isOpen')  # Remove target variable\n",
    "\n",
    "print(f\"Using {len(clustering_features)} features for clustering:\")\n",
    "for feature in clustering_features[:5]:  # Show first 5\n",
    "    print(f\"  - {feature}\")\n",
    "if len(clustering_features) > 5:\n",
    "    print(f\"  ... and {len(clustering_features) - 5} more\")\n",
    "\n",
    "# Prepare clustering data\n",
    "cluster_data = df[clustering_features].fillna(0)\n",
    "\n",
    "# Standardize the features (important for K-means)\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(cluster_data)\n",
    "\n",
    "print(f\"\\nData scaled and ready for clustering\")\n",
    "print(f\"Shape: {scaled_data.shape[0]} samples, {scaled_data.shape[1]} features\")\n",
    "\n",
    "# Determine optimal number of clusters using elbow method\n",
    "print(\"\\nFinding optimal number of clusters...\")\n",
    "\n",
    "inertias = []\n",
    "k_range = range(2, 11)\n",
    "\n",
    "for k in k_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    kmeans.fit(scaled_data)\n",
    "    inertias.append(kmeans.inertia_)\n",
    "\n",
    "# Plot elbow curve\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(k_range, inertias, 'bo-')\n",
    "plt.title('Elbow Method for Optimal k')\n",
    "plt.xlabel('Number of Clusters (k)')\n",
    "plt.ylabel('Inertia (Within-cluster sum of squares)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Add annotations for key points\n",
    "for i, inertia in enumerate(inertias):\n",
    "    if i % 2 == 0:  # Annotate every other point to avoid crowding\n",
    "        plt.annotate(f'{inertia:.0f}', (k_range[i], inertia), \n",
    "                    textcoords=\"offset points\", xytext=(0,10), ha='center')\n",
    "\n",
    "# Choose optimal k (for demonstration, we'll use 4)\n",
    "optimal_k = 4\n",
    "print(f\"Using k = {optimal_k} clusters for analysis\")\n",
    "\n",
    "# Perform clustering\n",
    "kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)\n",
    "cluster_labels = kmeans.fit_predict(scaled_data)\n",
    "\n",
    "# Add cluster labels to original data\n",
    "df_clustered = df.copy()\n",
    "df_clustered['Cluster'] = cluster_labels\n",
    "\n",
    "print(f\"\\nClustering completed!\")\n",
    "print(f\"Cluster distribution:\")\n",
    "cluster_counts = pd.Series(cluster_labels).value_counts().sort_index()\n",
    "for cluster, count in cluster_counts.items():\n",
    "    percentage = (count / len(cluster_labels)) * 100\n",
    "    print(f\"  Cluster {cluster}: {count} samples ({percentage:.1f}%)\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "cluster_counts.plot(kind='bar', color='skyblue')\n",
    "plt.title('Cluster Size Distribution')\n",
    "plt.xlabel('Cluster')\n",
    "plt.ylabel('Number of Customers')\n",
    "plt.xticks(rotation=0)\n",
    "\n",
    "# Add value labels on bars\n",
    "for i, count in enumerate(cluster_counts):\n",
    "    plt.text(i, count + max(cluster_counts)*0.01, str(count), \n",
    "             ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Preparing Data for Clustering\n",
    "We'll use K-means clustering to segment our data into meaningful groups."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Unsupervised Learning - Clustering\n",
    "\n",
    "**Unsupervised Learning** is like finding hidden patterns in data without knowing the \"right answer\" beforehand. In business, this could be:\n",
    "- Segmenting customers based on spending behavior\n",
    "- Identifying unusual transaction patterns\n",
    "- Grouping products by similarity\n",
    "\n",
    "We'll perform customer segmentation to help with targeted marketing strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze feature importance (works with Random Forest)\n",
    "if 'Random Forest' in results:\n",
    "    rf_model = results['Random Forest']['model']\n",
    "    \n",
    "    # Get feature importance\n",
    "    importance = rf_model.feature_importances_\n",
    "    feature_names = X.columns\n",
    "    \n",
    "    # Create a DataFrame for easier analysis\n",
    "    importance_df = pd.DataFrame({\n",
    "        'feature': feature_names,\n",
    "        'importance': importance\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    print(\"Top 10 Most Important Features for Business Success:\")\n",
    "    print(\"=\"*55)\n",
    "    \n",
    "    for i, (_, row) in enumerate(importance_df.head(10).iterrows(), 1):\n",
    "        print(f\"{i:2d}. {row['feature']:<25} {row['importance']:.4f}\")\n",
    "    \n",
    "    # Visualize feature importance\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    top_features = importance_df.head(10)\n",
    "    \n",
    "    plt.subplot(2, 1, 1)\n",
    "    bars = plt.bar(range(len(top_features)), top_features['importance'])\n",
    "    plt.title('Top 10 Feature Importance for Predicting Business Success')\n",
    "    plt.xlabel('Features')\n",
    "    plt.ylabel('Importance Score')\n",
    "    plt.xticks(range(len(top_features)), top_features['feature'], rotation=45, ha='right')\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for i, bar in enumerate(bars):\n",
    "        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.001, \n",
    "                f'{top_features.iloc[i][\"importance\"]:.3f}', \n",
    "                ha='center', va='bottom', fontsize=8)\n",
    "    \n",
    "    # Model performance comparison\n",
    "    plt.subplot(2, 1, 2)\n",
    "    model_names = list(results.keys())\n",
    "    accuracies = [results[name]['accuracy'] for name in model_names]\n",
    "    \n",
    "    bars = plt.bar(model_names, accuracies, color=['lightblue', 'lightgreen'])\n",
    "    plt.title('Model Performance Comparison')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.ylim(0, 1)\n",
    "    \n",
    "    # Add percentage labels on bars\n",
    "    for i, bar in enumerate(bars):\n",
    "        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
    "                f'{accuracies[i]:.1%}', ha='center', va='bottom')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Business insights\n",
    "    print(f\"\\nBusiness Insights:\")\n",
    "    print(\"-\" * 20)\n",
    "    print(f\"The Random Forest model identified the following key success factors:\")\n",
    "    \n",
    "    top_3_features = importance_df.head(3)\n",
    "    for i, (_, row) in enumerate(top_3_features.iterrows(), 1):\n",
    "        print(f\"{i}. {row['feature']} (importance: {row['importance']:.3f})\")\n",
    "    \n",
    "    print(f\"\\nThese features account for {top_3_features['importance'].sum():.1%} \"\n",
    "          f\"of the model's decision-making process.\")\n",
    "    \n",
    "else:\n",
    "    print(\"Feature importance analysis requires Random Forest model.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Feature Importance Analysis\n",
    "Understanding which factors most influence business success can provide valuable insights for decision-making."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train different classification models\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"Training {name}...\")\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    results[name] = {\n",
    "        'model': model,\n",
    "        'accuracy': accuracy,\n",
    "        'predictions': y_pred\n",
    "    }\n",
    "    \n",
    "    print(f\"{name} Accuracy: {accuracy:.3f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Model Comparison:\")\n",
    "print(\"-\" * 20)\n",
    "\n",
    "for name, result in results.items():\n",
    "    print(f\"{name}: {result['accuracy']:.1%} accuracy\")\n",
    "\n",
    "# Detailed analysis of the best model\n",
    "best_model_name = max(results.keys(), key=lambda x: results[x]['accuracy'])\n",
    "best_model = results[best_model_name]['model']\n",
    "best_predictions = results[best_model_name]['predictions']\n",
    "\n",
    "print(f\"\\nDetailed Analysis - {best_model_name}\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "# Classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, best_predictions, \n",
    "                          target_names=['Closed', 'Open']))\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, best_predictions)\n",
    "print(f\"\\nConfusion Matrix:\")\n",
    "print(\"Actual ‚Üí\")\n",
    "print(\"        Closed  Open\")\n",
    "print(f\"Closed    {cm[0,0]:4d}   {cm[0,1]:3d}\")\n",
    "print(f\"Open      {cm[1,0]:4d}   {cm[1,1]:3d}\")\n",
    "\n",
    "# Business interpretation\n",
    "correct_predictions = cm[0,0] + cm[1,1]\n",
    "total_predictions = cm.sum()\n",
    "print(f\"\\nBusiness Impact:\")\n",
    "print(f\"  Correctly identified {cm[1,1]} businesses that stayed open\")\n",
    "print(f\"  Correctly identified {cm[0,0]} businesses that closed\")\n",
    "print(f\"  Misclassified {cm[0,1]} closing businesses as staying open\")\n",
    "print(f\"  Misclassified {cm[1,0]} successful businesses as closing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Training Classification Models\n",
    "Let's train two different types of models to predict business success and compare their performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import machine learning libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Check if we have the target column\n",
    "if 'isOpen' not in df.columns:\n",
    "    print(\"Warning: 'isOpen' column not found. Creating a sample target for demonstration.\")\n",
    "    # Create a sample target based on some business logic\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    if len(numeric_cols) > 0:\n",
    "        # Create target based on above-median performance in first numeric column\n",
    "        df['isOpen'] = (df[numeric_cols[0]] > df[numeric_cols[0]].median()).astype(int)\n",
    "    else:\n",
    "        # Random target for demonstration\n",
    "        np.random.seed(42)\n",
    "        df['isOpen'] = np.random.choice([0, 1], size=len(df))\n",
    "\n",
    "# Prepare features for machine learning\n",
    "print(\"Preparing data for machine learning...\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Select numeric columns for features\n",
    "numeric_features = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "if 'isOpen' in numeric_features:\n",
    "    numeric_features.remove('isOpen')\n",
    "\n",
    "print(f\"Using {len(numeric_features)} numeric features:\")\n",
    "for feature in numeric_features:\n",
    "    print(f\"  - {feature}\")\n",
    "\n",
    "# Handle categorical columns if they exist\n",
    "categorical_features = df.select_dtypes(include=['object']).columns.tolist()\n",
    "processed_df = df.copy()\n",
    "\n",
    "if categorical_features:\n",
    "    print(f\"\\nProcessing {len(categorical_features)} categorical features...\")\n",
    "    label_encoders = {}\n",
    "    \n",
    "    for col in categorical_features[:3]:  # Limit to first 3 categorical columns\n",
    "        le = LabelEncoder()\n",
    "        processed_df[col + '_encoded'] = le.fit_transform(processed_df[col].fillna('Unknown'))\n",
    "        label_encoders[col] = le\n",
    "        numeric_features.append(col + '_encoded')\n",
    "        print(f\"  - Encoded {col}\")\n",
    "\n",
    "# Prepare X (features) and y (target)\n",
    "X = processed_df[numeric_features].fillna(0)  # Fill missing values with 0\n",
    "y = processed_df['isOpen']\n",
    "\n",
    "print(f\"\\nFinal feature set: {X.shape[1]} features\")\n",
    "print(f\"Target distribution:\")\n",
    "print(f\"  Open businesses: {sum(y == 1)} ({sum(y == 1)/len(y)*100:.1f}%)\")\n",
    "print(f\"  Closed businesses: {sum(y == 0)} ({sum(y == 0)/len(y)*100:.1f}%)\")\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"\\nData split complete:\")\n",
    "print(f\"  Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"  Testing set: {X_test.shape[0]} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Preparing Data for Machine Learning\n",
    "Before we can train a model, we need to prepare our data by selecting features and cleaning it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Supervised Learning - Classification\n",
    "\n",
    "**Supervised Learning** is like learning from examples with known answers. In business, this could be:\n",
    "- Predicting if a business will stay open based on financial indicators\n",
    "- Determining if a customer will default on a loan\n",
    "- Classifying transactions as fraudulent or legitimate\n",
    "\n",
    "We'll use the `isOpen` column as our target to predict business success."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualizations to understand our data better\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "fig.suptitle('Business Data Analysis Dashboard', fontsize=16, y=1.02)\n",
    "\n",
    "# Plot 1: Business Status Distribution (if exists)\n",
    "if 'isOpen' in df.columns:\n",
    "    status_counts = df['isOpen'].value_counts()\n",
    "    labels = ['Closed' if x == 0 else 'Open' for x in status_counts.index]\n",
    "    axes[0, 0].pie(status_counts.values, labels=labels, autopct='%1.1f%%', startangle=90)\n",
    "    axes[0, 0].set_title('Business Status Distribution')\n",
    "else:\n",
    "    # Alternative plot if isOpen doesn't exist\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    if len(numeric_cols) > 0:\n",
    "        axes[0, 0].hist(df[numeric_cols[0]], bins=20, alpha=0.7)\n",
    "        axes[0, 0].set_title(f'Distribution of {numeric_cols[0]}')\n",
    "\n",
    "# Plot 2: Spending distribution (first numeric column)\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "if len(numeric_cols) > 0:\n",
    "    spending_col = numeric_cols[0]\n",
    "    axes[0, 1].hist(df[spending_col], bins=20, alpha=0.7, color='skyblue')\n",
    "    axes[0, 1].set_title(f'Distribution of {spending_col}')\n",
    "    axes[0, 1].set_xlabel(spending_col)\n",
    "    axes[0, 1].set_ylabel('Frequency')\n",
    "\n",
    "# Plot 3: Correlation heatmap (if we have multiple numeric columns)\n",
    "if len(numeric_cols) >= 2:\n",
    "    correlation_matrix = df[numeric_cols[:5]].corr()  # Limit to 5 columns for readability\n",
    "    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0, ax=axes[1, 0])\n",
    "    axes[1, 0].set_title('Correlation Matrix')\n",
    "else:\n",
    "    axes[1, 0].text(0.5, 0.5, 'Not enough numeric\\ncolumns for correlation', \n",
    "                    ha='center', va='center', transform=axes[1, 0].transAxes)\n",
    "    axes[1, 0].set_title('Correlation Matrix')\n",
    "\n",
    "# Plot 4: Category analysis (if categorical columns exist)\n",
    "if len(categorical_cols) > 0:\n",
    "    cat_col = categorical_cols[0]\n",
    "    category_counts = df[cat_col].value_counts().head(10)  # Top 10 categories\n",
    "    category_counts.plot(kind='bar', ax=axes[1, 1])\n",
    "    axes[1, 1].set_title(f'Top Categories in {cat_col}')\n",
    "    axes[1, 1].set_xlabel(cat_col)\n",
    "    axes[1, 1].set_ylabel('Count')\n",
    "    axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "else:\n",
    "    axes[1, 1].text(0.5, 0.5, 'No categorical\\ncolumns found', \n",
    "                    ha='center', va='center', transform=axes[1, 1].transAxes)\n",
    "    axes[1, 1].set_title('Category Analysis')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Additional summary statistics\n",
    "print(\"\\nKey Business Metrics:\")\n",
    "print(\"=\" * 30)\n",
    "if len(numeric_cols) > 0:\n",
    "    for col in numeric_cols[:3]:  # Show top 3 numeric columns\n",
    "        print(f\"{col}:\")\n",
    "        print(f\"  Mean: {df[col].mean():.2f}\")\n",
    "        print(f\"  Std Dev: {df[col].std():.2f}\")\n",
    "        print(f\"  Range: {df[col].min():.2f} to {df[col].max():.2f}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Data Visualization\n",
    "Visual representations help us understand patterns in our business data more easily than looking at numbers alone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore categorical columns\n",
    "print(\"Unique values in categorical columns:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "categorical_cols = df.select_dtypes(include=['object']).columns\n",
    "for col in categorical_cols:\n",
    "    print(f\"{col}: {df[col].nunique()} unique values\")\n",
    "    print(f\"Values: {list(df[col].unique())}\")\n",
    "    print()\n",
    "\n",
    "# Check for missing values\n",
    "print(\"Missing values:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "\n",
    "# Business insights from the data\n",
    "print(\"Business Analysis:\")\n",
    "print(\"-\" * 20)\n",
    "\n",
    "if 'isOpen' in df.columns:\n",
    "    business_status = df['isOpen'].value_counts()\n",
    "    print(f\"Business Status Distribution:\")\n",
    "    for status, count in business_status.items():\n",
    "        percentage = (count / len(df)) * 100\n",
    "        print(f\"  {'Open' if status == 1 else 'Closed'}: {count} ({percentage:.1f}%)\")\n",
    "\n",
    "# Analyze spending patterns if spending columns exist\n",
    "spending_cols = [col for col in df.columns if 'spend' in col.lower() or 'amount' in col.lower()]\n",
    "if spending_cols:\n",
    "    print(f\"\\nSpending Analysis:\")\n",
    "    for col in spending_cols[:3]:  # Show first 3 spending columns\n",
    "        print(f\"{col}:\")\n",
    "        print(f\"  Average: ${df[col].mean():,.2f}\")\n",
    "        print(f\"  Median: ${df[col].median():,.2f}\")\n",
    "        print(f\"  Min: ${df[col].min():,.2f}\")\n",
    "        print(f\"  Max: ${df[col].max():,.2f}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Data Exploration and Analysis\n",
    "Let's explore our business data to understand customer patterns, spending behavior, and business performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('../data/w2--dataset.csv')\n",
    "\n",
    "print(\"Dataset loaded successfully!\")\n",
    "print(f\"Shape: {df.shape[0]} rows, {df.shape[1]} columns\")\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "\n",
    "# First look at the data\n",
    "print(\"First 5 rows:\")\n",
    "print(df.head())\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Column information:\")\n",
    "print(df.info())\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Basic statistics:\")\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Working with Data Using Pandas\n",
    "\n",
    "Now that we understand Python basics, let's learn how to work with real business data using Pandas - Python's most popular data analysis library. Think of Pandas as Excel, but much more powerful!\n",
    "\n",
    "### 2.1 Loading and Exploring Our Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions for common business calculations\n",
    "\n",
    "def calculate_roi(initial_investment, final_value):\n",
    "    \"\"\"Calculate Return on Investment (ROI) as a percentage\"\"\"\n",
    "    roi = ((final_value - initial_investment) / initial_investment) * 100\n",
    "    return roi\n",
    "\n",
    "def categorize_expense(amount):\n",
    "    \"\"\"Categorize expenses as Low, Medium, or High\"\"\"\n",
    "    if amount < 1000:\n",
    "        return \"Low\"\n",
    "    elif amount < 5000:\n",
    "        return \"Medium\"\n",
    "    else:\n",
    "        return \"High\"\n",
    "\n",
    "def calculate_break_even(fixed_costs, price_per_unit, variable_cost_per_unit):\n",
    "    \"\"\"Calculate break-even point in units\"\"\"\n",
    "    if price_per_unit <= variable_cost_per_unit:\n",
    "        return None  # No break-even possible\n",
    "    break_even_units = fixed_costs / (price_per_unit - variable_cost_per_unit)\n",
    "    return break_even_units\n",
    "\n",
    "# Using the functions\n",
    "print(\"ROI Analysis:\")\n",
    "investments = [\n",
    "    {\"name\": \"Project A\", \"initial\": 50000, \"final\": 65000},\n",
    "    {\"name\": \"Project B\", \"initial\": 30000, \"final\": 42000},\n",
    "    {\"name\": \"Project C\", \"initial\": 80000, \"final\": 85000}\n",
    "]\n",
    "\n",
    "for investment in investments:\n",
    "    roi = calculate_roi(investment[\"initial\"], investment[\"final\"])\n",
    "    print(f\"{investment['name']}: {roi:.1f}% ROI\")\n",
    "\n",
    "print(\"\\nExpense Categorization:\")\n",
    "expenses = [800, 2500, 15000, 450, 8500]\n",
    "for expense in expenses:\n",
    "    category = categorize_expense(expense)\n",
    "    print(f\"${expense:,} - {category}\")\n",
    "\n",
    "print(\"\\nBreak-Even Analysis:\")\n",
    "fixed_costs = 100000\n",
    "price_per_unit = 50\n",
    "variable_cost = 20\n",
    "\n",
    "break_even = calculate_break_even(fixed_costs, price_per_unit, variable_cost)\n",
    "if break_even:\n",
    "    print(f\"Fixed Costs: ${fixed_costs:,}\")\n",
    "    print(f\"Price per unit: ${price_per_unit}\")\n",
    "    print(f\"Variable cost per unit: ${variable_cost}\")\n",
    "    print(f\"Break-even point: {break_even:.0f} units\")\n",
    "    print(f\"Break-even revenue: ${break_even * price_per_unit:,.0f}\")\n",
    "else:\n",
    "    print(\"Break-even not possible - price too low!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 Functions - Reusable Code\n",
    "Functions are like formulas in Excel that you can use over and over again with different inputs. They help organize code and avoid repetition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For loops - processing multiple items\n",
    "departments = ['Sales', 'Marketing', 'Operations', 'Finance', 'HR']\n",
    "budgets = [150000, 80000, 120000, 100000, 60000]\n",
    "\n",
    "print(\"Department Budget Analysis:\")\n",
    "total_budget = 0\n",
    "for i in range(len(departments)):\n",
    "    dept = departments[i]\n",
    "    budget = budgets[i]\n",
    "    total_budget += budget\n",
    "    percentage = (budget / sum(budgets)) * 100\n",
    "    print(f\"{dept}: ${budget:,} ({percentage:.1f}% of total)\")\n",
    "\n",
    "print(f\"Total Budget: ${total_budget:,}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "\n",
    "# While loop - compound interest calculation\n",
    "principal = 10000\n",
    "interest_rate = 0.05\n",
    "target_amount = 15000\n",
    "years = 0\n",
    "\n",
    "print(f\"Growing ${principal:,} at {interest_rate*100}% annual interest:\")\n",
    "print(\"Year\\tAmount\")\n",
    "print(\"----\\t------\")\n",
    "\n",
    "current_amount = principal\n",
    "while current_amount < target_amount and years < 20:\n",
    "    print(f\"{years}\\t${current_amount:,.0f}\")\n",
    "    current_amount = current_amount * (1 + interest_rate)\n",
    "    years += 1\n",
    "\n",
    "print(f\"{years}\\t${current_amount:,.0f}\")\n",
    "print(f\"\\nIt takes {years} years to reach ${target_amount:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Loops - Repeating Tasks\n",
    "Loops help us perform the same task multiple times, like calculating monthly payments or processing multiple transactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If statements for business decisions\n",
    "credit_score = 720\n",
    "annual_income = 85000\n",
    "debt_to_income = 0.25\n",
    "\n",
    "# Credit approval logic\n",
    "if credit_score >= 750 and debt_to_income < 0.3:\n",
    "    approval = \"Approved - Excellent\"\n",
    "    interest_rate = 3.5\n",
    "elif credit_score >= 700 and debt_to_income < 0.4:\n",
    "    approval = \"Approved - Good\"\n",
    "    interest_rate = 4.2\n",
    "elif credit_score >= 650 and debt_to_income < 0.5:\n",
    "    approval = \"Approved - Fair\"\n",
    "    interest_rate = 5.8\n",
    "else:\n",
    "    approval = \"Declined\"\n",
    "    interest_rate = None\n",
    "\n",
    "print(\"Credit Application Review:\")\n",
    "print(f\"Credit Score: {credit_score}\")\n",
    "print(f\"Annual Income: ${annual_income:,}\")\n",
    "print(f\"Debt-to-Income Ratio: {debt_to_income*100:.1f}%\")\n",
    "print(f\"Decision: {approval}\")\n",
    "if interest_rate:\n",
    "    print(f\"Interest Rate: {interest_rate}%\")\n",
    "\n",
    "# Performance categorization\n",
    "quarterly_sales = 180000\n",
    "target_sales = 150000\n",
    "\n",
    "if quarterly_sales >= target_sales * 1.2:\n",
    "    performance = \"Excellent\"\n",
    "elif quarterly_sales >= target_sales * 1.1:\n",
    "    performance = \"Above Target\"\n",
    "elif quarterly_sales >= target_sales:\n",
    "    performance = \"Met Target\"\n",
    "else:\n",
    "    performance = \"Below Target\"\n",
    "\n",
    "print(f\"\\nQuarterly Performance:\")\n",
    "print(f\"Sales: ${quarterly_sales:,}\")\n",
    "print(f\"Target: ${target_sales:,}\")\n",
    "print(f\"Performance: {performance}\")\n",
    "print(f\"Achievement: {(quarterly_sales/target_sales)*100:.1f}% of target\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 If Statements - Making Decisions\n",
    "If statements help us make decisions based on conditions, like determining credit approval or categorizing performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionaries - like a lookup table\n",
    "company_info = {\n",
    "    'name': 'ABC Retail Corp',\n",
    "    'employees': 250,\n",
    "    'revenue': 2500000,\n",
    "    'founded': 2010,\n",
    "    'industry': 'Retail'\n",
    "}\n",
    "\n",
    "# Monthly expenses by category\n",
    "monthly_expenses = {\n",
    "    'Rent': 15000,\n",
    "    'Utilities': 3500,\n",
    "    'Supplies': 2800,\n",
    "    'Marketing': 8000,\n",
    "    'Insurance': 1200\n",
    "}\n",
    "\n",
    "print(\"Company Information:\")\n",
    "for key, value in company_info.items():\n",
    "    print(f\"{key.capitalize()}: {value}\")\n",
    "\n",
    "print(f\"\\nMonthly Expenses:\")\n",
    "total_expenses = 0\n",
    "for category, amount in monthly_expenses.items():\n",
    "    print(f\"{category}: ${amount:,}\")\n",
    "    total_expenses += amount\n",
    "\n",
    "print(f\"Total Monthly Expenses: ${total_expenses:,}\")\n",
    "\n",
    "# Accessing specific values\n",
    "print(f\"\\nThe company {company_info['name']} spends ${monthly_expenses['Marketing']:,} on marketing each month.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Dictionaries - Key-Value Pairs\n",
    "Dictionaries are like a two-column table where each key has a corresponding value. Perfect for storing related business information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lists - like columns in Excel\n",
    "monthly_sales = [45000, 52000, 48000, 61000, 55000, 49000]\n",
    "expense_categories = ['Rent', 'Utilities', 'Supplies', 'Marketing', 'Insurance']\n",
    "\n",
    "print(\"Monthly Sales:\")\n",
    "print(monthly_sales)\n",
    "print(f\"Total Sales: ${sum(monthly_sales):,}\")\n",
    "print(f\"Average Monthly Sales: ${sum(monthly_sales)/len(monthly_sales):,.0f}\")\n",
    "print(f\"Highest Month: ${max(monthly_sales):,}\")\n",
    "print(f\"Lowest Month: ${min(monthly_sales):,}\")\n",
    "\n",
    "print(\"\\nExpense Categories:\")\n",
    "print(expense_categories)\n",
    "print(f\"Number of categories: {len(expense_categories)}\")\n",
    "print(f\"First category: {expense_categories[0]}\")\n",
    "print(f\"Last category: {expense_categories[-1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Lists - Collections of Data\n",
    "Lists are like a column in a spreadsheet. They can store multiple values, such as monthly sales figures or expense categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables - like account balances\n",
    "revenue = 150000\n",
    "expenses = 120000\n",
    "tax_rate = 0.25\n",
    "\n",
    "# Basic calculations\n",
    "gross_profit = revenue - expenses\n",
    "net_profit = gross_profit * (1 - tax_rate)\n",
    "\n",
    "print(f\"Revenue: ${revenue:,}\")\n",
    "print(f\"Expenses: ${expenses:,}\")\n",
    "print(f\"Gross Profit: ${gross_profit:,}\")\n",
    "print(f\"Net Profit: ${net_profit:,}\")\n",
    "print(f\"Profit Margin: {(net_profit/revenue)*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 2: Business Problems ‚Üí Data Solutions\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand basic Python programming concepts  \n",
    "- Learn how to translate accounting/finance problems into data science tasks\n",
    "- Differentiate between supervised and unsupervised learning\n",
    "- Apply simple machine learning models to accounting datasets\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Introduction to Python Basics\n",
    "\n",
    "Before we dive into data analysis, let's cover essential Python concepts using accounting and business examples. Don't worry if you're new to programming - we'll start with the basics!\n",
    "\n",
    "### 1.1 Variables and Basic Calculations\n",
    "Variables are like labeled containers that store values. In accounting, think of them as account names that hold monetary values."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
